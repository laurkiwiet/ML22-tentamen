{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-23 11:44:10.847 | INFO     | tentamen.data.data_tools:get_file:61 - File /home/azureuser/code/ML22-tentamen/data/raw/ArabicTrain.txt already exists, skip download\n",
      "2023-01-23 11:44:10.850 | INFO     | tentamen.data.data_tools:get_file:61 - File /home/azureuser/code/ML22-tentamen/data/raw/ArabicTest.txt already exists, skip download\n",
      "2023-01-23 11:44:10.851 | INFO     | tentamen.data.datasets:get_arabic:34 - Loading data from /home/azureuser/code/ML22-tentamen/data/raw/ArabicTrain.txt and /home/azureuser/code/ML22-tentamen/data/raw/ArabicTest.txt\n",
      "2023-01-23 11:44:11.949 | INFO     | tentamen.data.datasets:get_arabic:50 - Returning trainstreamer, teststreamer\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 33\u001b[0m\n\u001b[1;32m     19\u001b[0m     configs \u001b[39m=\u001b[39m [\n\u001b[1;32m     20\u001b[0m         LinearConfig(\n\u001b[1;32m     21\u001b[0m             \u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39m13\u001b[39m, output\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, tunedir\u001b[39m=\u001b[39mpresets\u001b[39m.\u001b[39mlogdir, h1\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, h2\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, dropout\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m         )\n\u001b[1;32m     26\u001b[0m     ]\n\u001b[1;32m     28\u001b[0m rnn \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mRNN(\n\u001b[1;32m     29\u001b[0m     input_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     30\u001b[0m     hidden_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[1;32m     31\u001b[0m     num_layers\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[1;32m     32\u001b[0m     batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 33\u001b[0m out, hidden \u001b[39m=\u001b[39m rnn(x)\n\u001b[1;32m     34\u001b[0m out\u001b[39m.\u001b[39mshape, hidden\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from loguru import logger\n",
    "\n",
    "from tentamen.data import datasets\n",
    "from tentamen.model import Accuracy\n",
    "from tentamen.settings import presets\n",
    "from tentamen.train import trainloop\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.add(presets.logdir / \"01.log\")\n",
    "\n",
    "    trainstreamer, teststreamer = datasets.get_arabic(presets)\n",
    "\n",
    "    from tentamen.model import GRUmodel, Linear, GRUClassifier\n",
    "    from tentamen.settings import LinearConfig, GruConfig\n",
    "\n",
    "    configs = [\n",
    "        LinearConfig(\n",
    "            input=13, output=20, tunedir=presets.logdir, h1=100, h2=10, dropout=0.5\n",
    "        ),\n",
    "        GruConfig(\n",
    "            input=13, output=10,tunedir=presets.logdir, num_layers=2, hidden_size=32, dropout=0.2\n",
    "        )\n",
    "    ]\n",
    "\n",
    "rnn = torch.nn.RNN(\n",
    "    input_size=1,\n",
    "    hidden_size=10,\n",
    "    num_layers=3,\n",
    "    batch_first=True)\n",
    "out, hidden = rnn(x)\n",
    "out.shape, hidden.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from typing import Callable, Dict, Protocol\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "class GRUmodel(nn.Module):\n",
    "    def __init__(self, config: Dict) -> None:\n",
    "        super(GRUmodel, self).__init__()\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=config[\"input\"],\n",
    "            hidden_size=config[\"hidden_size\"],\n",
    "            dropout=config[\"dropout\"],\n",
    "            batch_first=True,\n",
    "            num_layers=config[\"num_layers\"],\n",
    "        )\n",
    "        self.linear1 = nn.Linear(config[\"hidden_size\"], config[\"output\"])\n",
    "        self.linear2 = nn.Linear(config[\"output\"], config[\"num_classes\"])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        print(\"the shape is\",x.shape)\n",
    "        x, _ = self.rnn(x)\n",
    "        last_step = x[:, -1, :].squeeze() \n",
    "        yhat = self.linear1(last_step)\n",
    "        yhat = self.relu(yhat)\n",
    "        yhat = self.linear2(yhat)\n",
    "        yhat = self.softmax(yhat)\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-23 11:15:11.138 | INFO     | tentamen.train:dir_add_timestamp:159 - Logging to /home/azureuser/code/ML22-tentamen/logs/20230123-1115\n",
      "  0%|\u001b[38;2;30;71;6m          \u001b[0m| 0/51 [00:00<?, ?it/s]\n",
      "  0%|\u001b[38;2;30;71;6m          \u001b[0m| 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape is torch.Size([128, 63, 13])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 11 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m     12\u001b[0m model_gru \u001b[39m=\u001b[39m GRUmodel(config_GRU)\n\u001b[1;32m     13\u001b[0m   \u001b[39m#  for config in configs:\u001b[39;00m\n\u001b[1;32m     14\u001b[0m       \u001b[39m#  model = Linear(config.dict())  # type: ignore\u001b[39;00m\n\u001b[1;32m     15\u001b[0m        \u001b[39m# model_gru = GRUmodel(config.dict())\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m trainedmodel \u001b[39m=\u001b[39m trainloop(\n\u001b[1;32m     18\u001b[0m             epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m     19\u001b[0m             model\u001b[39m=\u001b[39;49mmodel_gru,  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m             optimizer\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam,\n\u001b[1;32m     21\u001b[0m             learning_rate\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m,\n\u001b[1;32m     22\u001b[0m             loss_fn\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mCrossEntropyLoss(),\n\u001b[1;32m     23\u001b[0m             metrics\u001b[39m=\u001b[39;49m[Accuracy()],\n\u001b[1;32m     24\u001b[0m             train_dataloader\u001b[39m=\u001b[39;49mtrainstreamer\u001b[39m.\u001b[39;49mstream(),\n\u001b[1;32m     25\u001b[0m             test_dataloader\u001b[39m=\u001b[39;49mteststreamer\u001b[39m.\u001b[39;49mstream(),\n\u001b[1;32m     26\u001b[0m             log_dir\u001b[39m=\u001b[39;49mpresets\u001b[39m.\u001b[39;49mlogdir,\n\u001b[1;32m     27\u001b[0m             train_steps\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(trainstreamer),\n\u001b[1;32m     28\u001b[0m             eval_steps\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(teststreamer),\n\u001b[1;32m     29\u001b[0m         )\n\u001b[1;32m     31\u001b[0m timestamp \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m path \u001b[39m=\u001b[39m presets\u001b[39m.\u001b[39mmodeldir \u001b[39m/\u001b[39m (timestamp \u001b[39m+\u001b[39m presets\u001b[39m.\u001b[39mmodelname)\n",
      "File \u001b[0;32m~/code/ML22-tentamen/tentamen/train.py:121\u001b[0m, in \u001b[0;36mtrainloop\u001b[0;34m(epochs, model, optimizer, learning_rate, loss_fn, metrics, train_dataloader, test_dataloader, log_dir, train_steps, eval_steps, patience, factor, tunewriter)\u001b[0m\n\u001b[1;32m    118\u001b[0m     writer \u001b[39m=\u001b[39m SummaryWriter(log_dir\u001b[39m=\u001b[39mlog_dir)\n\u001b[1;32m    120\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs), colour\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m#1e4706\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 121\u001b[0m     train_loss \u001b[39m=\u001b[39m trainbatches(\n\u001b[1;32m    122\u001b[0m         model, train_dataloader, loss_fn, optimizer_, train_steps\n\u001b[1;32m    123\u001b[0m     )\n\u001b[1;32m    125\u001b[0m     metric_dict, test_loss \u001b[39m=\u001b[39m evalbatches(\n\u001b[1;32m    126\u001b[0m         model, test_dataloader, loss_fn, metrics, eval_steps\n\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    129\u001b[0m     scheduler\u001b[39m.\u001b[39mstep(test_loss)\n",
      "File \u001b[0;32m~/code/ML22-tentamen/tentamen/train.py:27\u001b[0m, in \u001b[0;36mtrainbatches\u001b[0;34m(model, traindatastreamer, loss_fn, optimizer, train_steps)\u001b[0m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     26\u001b[0m yhat \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m---> 27\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(yhat, y)\n\u001b[1;32m     28\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     29\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/tentamen-ODJr_Lwd-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/tentamen-ODJr_Lwd-py3.9/lib/python3.9/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/tentamen-ODJr_Lwd-py3.9/lib/python3.9/site-packages/torch/nn/functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3025\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3026\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 11 is out of bounds."
     ]
    }
   ],
   "source": [
    "config =  GruConfig(input=13, output=10,tunedir=presets.logdir, num_layers=2, hidden_size=32, dropout=0.2)\n",
    "\n",
    "config_GRU = {\n",
    "        \"input\": 13,\n",
    "        \"hidden_size\": 64,\n",
    "        \"dropout\": 0,\n",
    "        \"num_layers\": 1,\n",
    "        \"output\": 32,\n",
    "        \"num_classes\": 10\n",
    "    }\n",
    "\n",
    "model_gru = GRUmodel(config_GRU)\n",
    "  #  for config in configs:\n",
    "      #  model = Linear(config.dict())  # type: ignore\n",
    "       # model_gru = GRUmodel(config.dict())\n",
    "\n",
    "trainedmodel = trainloop(\n",
    "            epochs=10,\n",
    "            model=model_gru,  # type: ignore\n",
    "            optimizer=torch.optim.Adam,\n",
    "            learning_rate=1e-3,\n",
    "            loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "            metrics=[Accuracy()],\n",
    "            train_dataloader=trainstreamer.stream(),\n",
    "            test_dataloader=teststreamer.stream(),\n",
    "            log_dir=presets.logdir,\n",
    "            train_steps=len(trainstreamer),\n",
    "            eval_steps=len(teststreamer),\n",
    "        )\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "path = presets.modeldir / (timestamp + presets.modelname)\n",
    "logger.info(f\"save model to {path}\")\n",
    "torch.save(trainedmodel, path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tentamen-ODJr_Lwd-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Nov 19 2022, 09:58:14) \n[GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fbef76b2a22da125a2b6c0bca10e57c7eee6aebc77cdd3b59e3c27f177e62e93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
